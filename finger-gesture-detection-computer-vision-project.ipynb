{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 file \"Signs_Data_Training.h5\" (mode r)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opening H5 file\n",
    "import h5py # https://www.h5py.org\n",
    "\n",
    "training_data = h5py.File(\"./datasets/Signs_Data_Training.h5\", \"r\")\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['list_classes', 'train_set_x', 'train_set_y']>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (1080, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "# X_train = np.array(training_data['train_set_x'])\n",
    "# y_train = np.array(training_data['train_set_y'])\n",
    "\n",
    "train_data = np.array(training_data['train_set_x'])\n",
    "train_labels = np.array(training_data['train_set_y'])\n",
    "print(\"Train data shape: \", train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1080 photos of 64x64 size, 3 colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose data into the format Torch wants - (n_channels, width, height)\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 3, 64, 64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://numpy.org/doc/stable/reference/generated/numpy.transpose.html\n",
    "\n",
    "train_data = np.transpose(train_data, axes=(0, 3, 1, 2))\n",
    "\n",
    "\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Taken from Yann's paper\n",
    "class LeNet5(torch.nn.Module):\n",
    "    def __init__(self, input_height, input_width, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # C1. Put padding bcs MNIST had 32x32 data. We have 28x28\n",
    "        self.C1 = torch.nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, padding=2)\n",
    "\n",
    "        # S2\n",
    "        self.S2 = torch.nn.AvgPool2d(kernel_size=2)\n",
    "\n",
    "        # C3\n",
    "        self.C3 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "\n",
    "        # S4\n",
    "        self.S4 = torch.nn.AvgPool2d(kernel_size=4) # Edited this due to our data being 64x64\n",
    "\n",
    "        # C5\n",
    "        self.C5 = torch.nn.Conv2d(in_channels=16, out_channels=120, kernel_size=7) # Edited this due to our data being 64x64\n",
    "\n",
    "        # F6\n",
    "        self.F6 = torch.nn.Linear(in_features= 120, out_features=84)\n",
    "        # self.layer4 = torch.nn.Conv2d(in_channels=64, out_channels=16, kernel_size=1)\n",
    "\n",
    "        # F7\n",
    "        self.F7 = torch.nn.Linear(in_features= 84, out_features=n_classes)\n",
    "\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        ### forward pass module\n",
    "        # x = torch.unsqueeze(x, 1)\n",
    "\n",
    "        ### feed forward function\n",
    "        # print(\"x.shape: \", x.shape)\n",
    "        \n",
    "        # PyTorch expects (Batch_size, channel_nr, width, height)\n",
    "        \n",
    "        l1out = self.C1(x)\n",
    "        l1out = torch.nn.Tanh()(l1out)\n",
    "        \n",
    "        # print(\"l1out.shape: After C1 and TanH\", l1out.shape)\n",
    "\n",
    "        # AvgPooling\n",
    "        l2out = self.S2(l1out)\n",
    "        \n",
    "        # print(\"l2out.shape: after AvgPooling \", l2out.shape)\n",
    "\n",
    "        l3out = self.C3(l2out)\n",
    "        l3out = torch.nn.Tanh()(l3out)\n",
    "        \n",
    "        # print(\"l3out.shape: After C3 and TanH\", l3out.shape)\n",
    "\n",
    "        # AvgPooling\n",
    "        l4out = self.S4(l3out)\n",
    "        \n",
    "        # print(\"l4out.shape, After Avg Pooling: \", l4out.shape)\n",
    "\n",
    "        l5out = self.C5(l4out)\n",
    "        l5out = torch.nn.Tanh()(l5out)\n",
    "        \n",
    "        # print(\"l5out.shape: after C5 and Tanh: \", l5out.shape)\n",
    "\n",
    "        l6out = self.F6(np.squeeze(l5out)) # squeeze bcs (120,1) vector to (120,) - a numpy broadcast thing\n",
    "        \n",
    "        # print(\"l6out.shape: after F6 and squeeze \", l6out.shape)\n",
    "        \n",
    "        l7out = self.F7(l6out)\n",
    "        \n",
    "        # print(\"l7out.shape: After F7 \", l7out.shape)\n",
    "\n",
    "        return l7out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train into Train and Dev. Test set is in another HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Cross-Validation/Dev set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (864, 3, 64, 64)\n",
      "y_train.shape:  (864,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train.shape: \", X_train.shape)\n",
    "print(\"y_train.shape: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/installing-pytorch-on-apple-m1-chip-with-gpu-acceleration-3351dc44d67c\n",
    "\n",
    "# this ensures that the current MacOS version is at least 12.3+\n",
    "print(torch.backends.mps.is_available())\n",
    "\n",
    "# this ensures that the current current PyTorch installation was built with MPS activated.\n",
    "print(torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.device(\"mps\") # Disable this if not on apple silicon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On step 0:\tTrain loss 1.7829965353012085\t|\tDev acc is 0.16666666666666666\n",
      "On step 100:\tTrain loss 0.43339401483535767\t|\tDev acc is 0.7314814814814815\n",
      "On step 200:\tTrain loss 0.21096572279930115\t|\tDev acc is 0.8425925925925926\n",
      "On step 300:\tTrain loss 0.06984391063451767\t|\tDev acc is 0.9027777777777778\n",
      "On step 400:\tTrain loss 0.033883873373270035\t|\tDev acc is 0.9166666666666666\n",
      "On step 500:\tTrain loss 0.029265256598591805\t|\tDev acc is 0.9212962962962963\n",
      "On step 600:\tTrain loss 0.00944521464407444\t|\tDev acc is 0.9305555555555556\n",
      "On step 700:\tTrain loss 0.005216752178966999\t|\tDev acc is 0.9259259259259259\n",
      "On step 800:\tTrain loss 0.004262969363480806\t|\tDev acc is 0.9351851851851852\n",
      "On step 900:\tTrain loss 0.0029565959703177214\t|\tDev acc is 0.9305555555555556\n",
      "On step 1000:\tTrain loss 0.001324393437243998\t|\tDev acc is 0.9305555555555556\n",
      "On step 1100:\tTrain loss 0.0032544159330427647\t|\tDev acc is 0.9305555555555556\n",
      "On step 1200:\tTrain loss 0.0021152303088456392\t|\tDev acc is 0.9351851851851852\n",
      "On step 1300:\tTrain loss 0.0010840834584087133\t|\tDev acc is 0.9351851851851852\n",
      "On step 1400:\tTrain loss 0.001007622224278748\t|\tDev acc is 0.9351851851851852\n",
      "On step 1500:\tTrain loss 0.0011260408209636807\t|\tDev acc is 0.9351851851851852\n",
      "On step 1600:\tTrain loss 0.0006244588876143098\t|\tDev acc is 0.9351851851851852\n",
      "On step 1700:\tTrain loss 0.0006229858263395727\t|\tDev acc is 0.9351851851851852\n",
      "On step 1800:\tTrain loss 0.0005138601991347969\t|\tDev acc is 0.9351851851851852\n",
      "On step 1900:\tTrain loss 0.0005344423116184771\t|\tDev acc is 0.9351851851851852\n",
      "Training completed, saving model at {model_savepath}\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from utils.accuracies import dev_acc_and_loss, approx_train_acc_and_loss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Modified From Ariel's EN.601.675 HW3\n",
    "        \n",
    "LOG_DIR = \".\"\n",
    "MODEL_SAVE_DIR = \".\"\n",
    "LEARNING_RATE = 0.0004 #arguments.get('lr')\n",
    "BATCH_SIZE = 100 # 250 #arguments.get('bs')\n",
    "EPOCHS = 2000 # arguments.get('epochs')\n",
    "DATE_PREFIX = datetime.datetime.now().strftime('%Y%m%d%H%M')\n",
    "\n",
    "\n",
    "TRAIN_IMAGES = X_train\n",
    "TRAIN_LABELS = y_train\n",
    "DEV_IMAGES = X_dev\n",
    "DEV_LABELS = y_dev\n",
    "\n",
    "### format dataset to the appropriate shape/dimensions necessary to be input into the model\n",
    "\n",
    "\n",
    "n_train_imgs = TRAIN_IMAGES.shape[0]\n",
    "HEIGHT = TRAIN_IMAGES.shape[1]\n",
    "WIDTH = TRAIN_IMAGES.shape[2]\n",
    "#  this will not be correct if not all classes are present in training\n",
    "#   But if classes are entirely missing from training, we cannot possibly hope to do well on them\n",
    "N_CLASSES = len(np.unique(TRAIN_LABELS))\n",
    "# raise NotImplementedError\n",
    "\n",
    "### Normalize \n",
    "train_imgs = (TRAIN_IMAGES - np.mean(TRAIN_IMAGES))/ np.std(TRAIN_IMAGES)\n",
    "dev_imgs = (DEV_IMAGES - np.mean(DEV_IMAGES))/ np.std(DEV_IMAGES)\n",
    "\n",
    "### change depending on your model's instantiation\n",
    "\n",
    "model = LeNet5(input_height = HEIGHT, input_width= WIDTH,\n",
    "                  n_classes=N_CLASSES)\n",
    "\n",
    "\n",
    "### (OPTIONAL) : you can change the choice of optimizer here if you wish.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "\n",
    "for step in range(EPOCHS):\n",
    "    i = np.random.choice(train_imgs.shape[0], size=BATCH_SIZE, replace=False)\n",
    "    x = torch.from_numpy(np.squeeze(train_imgs[i]).astype(np.float32))\n",
    "    y = torch.from_numpy(np.squeeze(TRAIN_LABELS[i].astype(int)))\n",
    "\n",
    "\n",
    "    # Forward pass: Get logits for x\n",
    "    logits = model(x)\n",
    "    # Compute loss\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # log model performance every 100 epochs\n",
    "    if step % 100 == 0:\n",
    "        train_acc, train_loss = approx_train_acc_and_loss(model, train_imgs, TRAIN_LABELS)\n",
    "        dev_acc, dev_loss = dev_acc_and_loss(model, dev_imgs, DEV_LABELS)\n",
    "        step_metrics = {\n",
    "            'step': step, \n",
    "            'train_loss': loss.item(), \n",
    "            'train_acc': train_acc,\n",
    "            'dev_loss': dev_loss,\n",
    "            'dev_acc': dev_acc\n",
    "        }\n",
    "\n",
    "        print(f\"On step {step}:\\tTrain loss {train_loss}\\t|\\tDev acc is {dev_acc}\")\n",
    "        # logger.writerow(step_metrics)\n",
    "# LOGFILE.close()\n",
    "\n",
    "# Save model\n",
    "model_savepath = os.path.join(MODEL_SAVE_DIR,f\"{DATE_PREFIX}_bestmodel.pt\")\n",
    "\n",
    "\n",
    "print(\"Training completed, saving model at {model_savepath}\")\n",
    "torch.save(model, model_savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = h5py.File(\"./datasets/Signs_Data_Testing.h5\", \"r\")\n",
    "TEST_IMAGES = test_data[\"test_set_x\"]\n",
    "test_labels = test_data[\"test_set_y\"] # Do not train on these!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose into correct shape for PyTorch\n",
    "\n",
    "TEST_IMAGES = np.transpose(TEST_IMAGES, axes=(0, 3, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 3, 64, 64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_IMAGES.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 5, 1, 0, 3, 1, 5, 1, 5, 1, 3, 1, 1]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = torch.load(WEIGHTS_FILE) # Load model parameters from file to avoid retraining\n",
    "\n",
    "predictions = []\n",
    "testSetMean = TEST_IMAGES.mean()\n",
    "testSetStd = TEST_IMAGES.std()\n",
    "for test_case in TEST_IMAGES:\n",
    "\n",
    "    ### normalization schemes\n",
    "    test_case = (test_case - testSetMean) / testSetStd\n",
    "\n",
    "    # test_case = test_case.reshape(1,28,28)\n",
    "\n",
    "\n",
    "    x = torch.from_numpy(test_case.astype(np.float32))\n",
    "    # x = x.view(1,-1)\n",
    "    logits = model(x)\n",
    "    logits = logits.reshape(-1, 6)\n",
    "    pred = torch.max(logits, 1)[1]\n",
    "    predictions.append(pred.item())\n",
    "\n",
    "predictions[:15] # First 15 predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.95\n"
     ]
    }
   ],
   "source": [
    "from utils.accuracies import accuracy\n",
    "\n",
    "print(\"Test accuracy: \", accuracy(np.array(predictions), np.array(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
